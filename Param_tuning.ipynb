{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_PATH = os.getcwd()\n",
    "DATA_PATH = '/data/'\n",
    "VEC_PATH = '/wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data\n",
    "from load_data import create_weights, create_emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## added three cat_mode into model function\n",
    "import models_yi\n",
    "from models_yi import LogisticRegression, NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added training accuracy, need to add training and validation loss later\n",
    "import training_yi\n",
    "from training_yi import acc, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 100\n",
    "MAX_SENTENCE_LENGTH = 30\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASS = 20\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load raw data sets\n",
    "snli_train = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_train.tsv\", sep='\\t')\n",
    "snli_val = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_val.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess raw date sets\n",
    "train_data = load_data.prepare_data(snli_train)\n",
    "val_data = load_data.prepare_data(snli_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracking accuracy and parameters\n",
    "lr_train_acc = []\n",
    "nn_train_acc = []\n",
    "lr_val_acc = []\n",
    "nn_val_acc = []\n",
    "lr_param = []\n",
    "nn_param = []\n",
    "best_lr = 0\n",
    "best_nn = 0\n",
    "best_lr_param = None\n",
    "best_nn_param = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size:10000, Embed_dim:200, cat_mode:DIRECT\n",
      "Starting epoch 0\n",
      "Epoch: [1/10], Step: [500/3125],Training Acc: 55.639, Validation Acc: 54.6, Time: 32.8288369178772 sec\n",
      "Epoch: [1/10], Step: [1000/3125],Training Acc: 55.766, Validation Acc: 53.4, Time: 70.76151084899902 sec\n",
      "Epoch: [1/10], Step: [1500/3125],Training Acc: 60.774, Validation Acc: 58.5, Time: 113.03145503997803 sec\n",
      "Epoch: [1/10], Step: [2000/3125],Training Acc: 59.124, Validation Acc: 57.0, Time: 156.93732380867004 sec\n",
      "Epoch: [1/10], Step: [2500/3125],Training Acc: 57.667, Validation Acc: 54.5, Time: 201.06663298606873 sec\n",
      "Epoch: [1/10], Step: [3000/3125],Training Acc: 65.066, Validation Acc: 60.8, Time: 244.7548167705536 sec\n",
      "End of epoch 1, Training Acc: 64.698,Validation Acc: 60.3, Time: 268.79166293144226 sec\n",
      "New best model found, saving at model_lr.pt\n",
      "\n",
      "Starting epoch 1\n"
     ]
    }
   ],
   "source": [
    "for VOCAB_SIZE in [10000, 20000, 40000]:\n",
    "    vectors = pkl.load(open('pickle/'+str(VOCAB_SIZE)+'_vectors.pkl', 'rb'))\n",
    "    id2token = pkl.load(open('pickle/'+str(VOCAB_SIZE)+'_id2token.pkl', 'rb'))\n",
    "    token2id = pkl.load(open('pickle/'+str(VOCAB_SIZE)+'_token2id.pkl', 'rb'))\n",
    "    ## Convert to token lists to lists of corresponding indices\n",
    "    indiced_train_data, train_target = load_data.token2index_dataset(train_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "    indiced_val_data, val_target = load_data.token2index_dataset(val_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "    train_dataset = load_data.SNLIDataset(indiced_train_data, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "    val_dataset = load_data.SNLIDataset(indiced_val_data, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "    for embed_dim in [200,300,500]:\n",
    "        torch.manual_seed(embed_dim)\n",
    "        num_embed = len(set(id2token))\n",
    "        emb_layer = nn.Embedding(num_embed, embed_dim)\n",
    "        '''the following line is used for pre-trained embedding'''\n",
    "        \n",
    "        #embed_layer, num_embed, embed_dim = create_emb_layer(create_weights(vectors, id2token), non_trainable = True)\n",
    "        for cat_mode in [\"DIRECT\",\"MUL\",\"SUB\"]:\n",
    "            print('Vocab_size:{}, Embed_dim:{}, cat_mode:{}'.format(VOCAB_SIZE, embed_dim, cat_mode))\n",
    "            \n",
    "            criterion =nn.NLLLoss()\n",
    "            #train logistic regression model\n",
    "            lr_model = LogisticRegression(emb_layer, embed_dim, NUM_CLASS, cat_mode)\n",
    "            optimizer = optim.Adam(lr_model.parameters(), lr = LEARNING_RATE)\n",
    "            param, train_acc, val_acc = train_model(model = lr_model,train_loader = train_loader, val_loader=val_loader, optimizer=optimizer, criterion=criterion, n_epochs=NUM_EPOCHES,save_file = 'model_lr.pt')\n",
    "            lr_train_acc.append(train_acc)\n",
    "            lr_val_acc.append(val_acc)\n",
    "            lr_param.append(param)\n",
    "            if val_acc > best_lr:\n",
    "                best_lr = val_acc\n",
    "                best_lr_param = param\n",
    "                torch.save(param.state_dict(),'model_lr.pt')\n",
    "                print(\"New best LR model found after 10 epochs, saving at model_lr.pt\")\n",
    "                        \n",
    "                 \n",
    "                    \n",
    "            #train neural network model\n",
    "            nn_model = NeuralNetwork(emb_layer, embed_dim, NUM_CLASS, HIDDEN_DIM, cat_mode)\n",
    "            optimizer = optim.Adam(nn_model.parameters(), lr = LEARNING_RATE)\n",
    "            param, train_acc, val_acc = train_model(model = nn_model,train_loader = train_loader, val_loader=val_loader, optimizer=optimizer, criterion=criterion, n_epochs=NUM_EPOCHES,save_file = 'model_nn.pt')\n",
    "            nn_train_acc.append(train_acc)\n",
    "            nn_val_acc.append(val_acc)\n",
    "            nn_param.append(param)\n",
    "            if val_acc > best_nn:\n",
    "                best_nn = val_acc\n",
    "                best_nn_param = param\n",
    "                torch.save(param.state_dict(),'model_nn.pt')\n",
    "                print(\"New best NN model found after 10 epochs, saving at model_nn.pt\") \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training and validation accuracy into csv files, need to add loss            \n",
    "with open('lr_model.csv', 'ab') as f:\n",
    "    np.savetxt(f, lr_train_acc, lr_val_acc ,delimiter=\",\")    \n",
    "    \n",
    "with open('nn_model.csv', 'ab') as f:\n",
    "    np.savetxt(f, nn_train_acc, nn_val_acc ,delimiter=\",\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame reporting\n",
    "dat_lr = {'Vocab_size':np.repeat([10000, 20000, 40000],9), 'Embed_dim':np.tile(np.repeat([200,300,500],3),3),'Cat_mode':['DIRECT','MUL','SUB']*9,'train_acc':lr_train_acc, 'val_acc':lr_val_acc}\n",
    "df_lr = pd.DataFrame(dat_lr) \n",
    "\n",
    "dat_nn = {'Vocab_size':np.repeat([10000, 20000, 40000],9), 'Embed_dim':np.tile(np.repeat([200,300,500],3),3),'Cat_mode':['DIRECT','MUL','SUB']*9,'train_acc':nn_train_acc, 'val_acc':nn_val_acc}\n",
    "df_nn = pd.DataFrame(dat_nn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_PATH = os.getcwd()\n",
    "DATA_PATH = '/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load raw data sets\n",
    "snli_train = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_train.tsv\", sep='\\t')\n",
    "snli_val = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_val.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess raw datasets\n",
    "train_data = load_data.prepare_data(snli_train)\n",
    "val_data = load_data.prepare_data(snli_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASSIFIER CLASSES\n",
    "\n",
    "class LRClassifier(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        n_in: Number of features\n",
    "        n_out: Number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up out linear layer. This initializes the weights\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        \n",
    "        # Explicitly initialize the weights with the initialization\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input data [N, k]\n",
    "        ---\n",
    "        Returns: log probabilities of each class [N, c]\n",
    "        \"\"\"\n",
    "        # Apply the linear function to get our logit (real numbers)\n",
    "        logit = self.linear(x)\n",
    "        \n",
    "        # Apply log_softmax to get logs of normalized probabilities\n",
    "        return F.log_softmax(logit, dim=1)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # Use some specific initialization schemes\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.uniform_(self.linear.bias)\n",
    "\n",
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, n_in, h_s, n_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_in,h_s)\n",
    "        self.linear2 = nn.Linear(h_s,h_s)\n",
    "        self.linear3 = nn.Linear(h_s,n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        return F.log_softmax(x)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENCODER CLASS\n",
    "class BOWEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, class_in, is_pretrained):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BOWEncoder, self).__init__()\n",
    "        if is_pretrained:\n",
    "            weights_matrix = self.create_weights(vectors, id2token)\n",
    "            self.embed  = self.create_emb_layer(weights_matrix, is_pretrained)\n",
    "        else:\n",
    "            self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,class_in)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "    \n",
    "    def create_weights(self, vectors, id2token):\n",
    "        '''Create weights metrics from vectors and id2token\n",
    "        Returns:\n",
    "        weights_matrix: torch.Tensor, dimension of (vocab size x embedding dim)\n",
    "        '''\n",
    "        weights_matrix = torch.from_numpy(np.array([vectors[id2token[i]] for i in range(2, len(id2token))]))\n",
    "        zero = torch.zeros(2, weights_matrix.size()[1], dtype=torch.float64)\n",
    "        weights_matrix = torch.cat([zero, weights_matrix])\n",
    "        return weights_matrix\n",
    "\n",
    "    \n",
    "    def create_emb_layer(self, weights_matrix, non_trainable=False):\n",
    "        '''Create embedding layer that's used in a PyTorch model\n",
    "        Returns:\n",
    "        emb_layer: nn.Embedding()\n",
    "        num_embeddings: int\n",
    "        embedding_dim: int\n",
    "        '''\n",
    "        num_embeddings, embedding_dim = weights_matrix.size()\n",
    "        emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "        if non_trainable:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        return emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder + NN Classifier model\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, n_in, h_s, n_out, combine_mode, is_pretrained):\n",
    "        super().__init__()\n",
    "        self.encoder = BOWEncoder(vocab_size, embed_dim, n_in, is_pretrained)\n",
    "        self.combine_mode = combine_mode\n",
    "        if combine_mode == 'DIRECT':\n",
    "            n_in = n_in * 2;\n",
    "        self.classifier = NNClassifier(n_in,h_s, n_out)\n",
    "    \n",
    "    def forward(self, premise, len_premise, hypothesis, len_hypo):\n",
    "        premise = self.encoder(premise, len_premise)\n",
    "        hypothesis = self.encoder(hypothesis, len_hypo)\n",
    "        if self.combine_mode == 'DIRECT':\n",
    "            x = torch.cat((premise, hypothesis),1)\n",
    "        elif self.combine_mode == 'MUL':\n",
    "            x = torch.mul(premise, hypothesis)\n",
    "        elif self.combine_mode == 'SUB':\n",
    "            x = torch.sub(premise, hypothesis)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder + LR Classifier model\n",
    "class LRModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, n_in, n_out, combine_mode, is_pretrained):\n",
    "        super().__init__()\n",
    "        self.encoder = BOWEncoder(vocab_size, embed_dim, n_in, is_pretrained)\n",
    "        self.combine_mode = combine_mode\n",
    "        if combine_mode == 'DIRECT':\n",
    "            n_in = n_in * 2;\n",
    "        self.classifier = LRClassifier(n_in, n_out)\n",
    "    \n",
    "    def forward(self, premise, len_premise, hypothesis, len_hypo):\n",
    "        premise = self.encoder(premise, len_premise)\n",
    "        hypothesis = self.encoder(hypothesis, len_hypo)\n",
    "        if self.combine_mode == 'DIRECT':\n",
    "            x = torch.cat((premise, hypothesis),1)\n",
    "        elif self.combine_mode == 'MUL':\n",
    "            x = torch.mul(premise, hypothesis)\n",
    "        elif cself.ombine_mode == 'SUB':\n",
    "            x = torch.sub(premise, hypothesis)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "num_class = 3\n",
    "batch_size = 32\n",
    "h_s = 100 # size of hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 30\n",
    "SAVE_FOLDER = os.path.join('models', 'snli')\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)\n",
    "\n",
    "# # For hyperparameter tuning\n",
    "# VOCAB_SIZES = [5000, 10000, 20000, 40000,50000]\n",
    "# EMB_DIMS = [50, 100,200,300,500]\n",
    "# CAT_MODES = [\"DIRECT\",\"MUL\",\"SUB\"]\n",
    "# MODEL_TYPES = ['log-reg', 'neural-net']\n",
    "# is_pretrained = False\n",
    "\n",
    "# For frozen embeddings\n",
    "VOCAB_SIZES = [100000]\n",
    "EMB_DIMS = [300]\n",
    "CAT_MODES = [\"DIRECT\"]\n",
    "MODEL_TYPES = ['neural-net']\n",
    "is_pretrained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP FOR SNLI DATA\n",
    "for vocab_size in VOCAB_SIZES:\n",
    "    # Load datasets\n",
    "    vectors = pkl.load(open('pickle/'+str(vocab_size)+'_vectors.pkl', 'rb'))\n",
    "    id2token = pkl.load(open('pickle/'+str(vocab_size)+'_id2token.pkl', 'rb'))\n",
    "    token2id = pkl.load(open('pickle/'+str(vocab_size)+'_token2id.pkl', 'rb'))\n",
    "    ## Convert to token lists to lists of corresponding indices\n",
    "    indiced_train_data, train_target = load_data.token2index_dataset(train_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "    indiced_val_data, val_target = load_data.token2index_dataset(val_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "    train_dataset = load_data.SNLIDataset(indiced_train_data, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "    val_dataset = load_data.SNLIDataset(indiced_val_data, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "    \n",
    "    for embed_dim in EMB_DIMS: \n",
    "        n_in = len(id2token)\n",
    "        \n",
    "        for cat_mode in CAT_MODES:\n",
    "            \n",
    "            for model_str in MODEL_TYPES:\n",
    "                print('Vocab_size:{}, Embed_dim:{}, cat_mode:{}, Classifier:{}'.format(vocab_size, embed_dim, cat_mode, model_str))\n",
    "                filename = '{}_{}_{}_{}.pt'.format(vocab_size, embed_dim, cat_mode, model_str)\n",
    "                save_path = os.path.join(SAVE_FOLDER, filename)\n",
    "                if model_str == 'neural-net':\n",
    "                    model = NNModel(vocab_size, embed_dim, n_in , h_s, num_class, cat_mode, is_pretrained)\n",
    "                elif model_str == 'log-reg':\n",
    "                    model = LRModel(vocab_size, embed_dim, n_in, num_class, cat_mode, is_pretrained)                \n",
    "\n",
    "                model.to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "                num_epochs = 10\n",
    "                best_accuracy = 0\n",
    "                \n",
    "                for epoch in range(num_epochs):\n",
    "                    for i, (premise, len_premise, hypothesis, len_hypo, labels) in enumerate(train_loader):\n",
    "                        model.train()\n",
    "                        # Load samples\n",
    "                        premise = premise.to(device)\n",
    "                        hypothesis = hypothesis.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(premise, len_premise, hypothesis, len_hypo)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if i > 0 and i % 500 == 0:\n",
    "                            train_loss = loss.data.item()\n",
    "\n",
    "                            model.eval()\n",
    "                            correct = 0\n",
    "                            total = 0\n",
    "\n",
    "                            for premise_val, len_premise_val, hypothesis_val, len_hypo_val, labels_val in val_loader:\n",
    "                                # Load samples\n",
    "                                premise_val = premise_val.to(device)\n",
    "                                hypothesis_val = hypothesis_val.to(device)\n",
    "                                labels_val = labels_val.to(device)\n",
    "\n",
    "                                outputs_val = model(premise_val, len_premise_val, hypothesis_val, len_hypo_val)\n",
    "                \n",
    "                                val_loss = criterion(outputs_val, labels_val)\n",
    "                                predicted = outputs_val.max(1, keepdim=True)[1]\n",
    "                                total += labels_val.size(0)\n",
    "                                correct += predicted.eq(labels_val.view_as(predicted)).sum().item()\n",
    "            \n",
    "                            accuracy = 100. * correct / total\n",
    "#                             print('Iter: {} | Train Loss: {} | Val Loss: {} | Val Accuracy: {}'.format(i, train_loss, val_loss.item(), round(accuracy, 2)))              \n",
    "                            # Save model when accuracy beats best accuracy\n",
    "                            if accuracy > best_accuracy:\n",
    "                                best_accuracy = accuracy\n",
    "                                torch.save(model.state_dict(), save_path)\n",
    "                    print('Epoch: {} | Train Loss: {} | Val Loss: {} | Val Accuracy: {}'.format((epoch+1), train_loss, val_loss.item(), round(accuracy, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIMS = [300]\n",
    "VOCAB_SIZES = [100000]\n",
    "CAT_MODES = [\"DIRECT\"]\n",
    "vocab_size = 100000\n",
    "vectors = pkl.load(open('pickle/'+str(vocab_size)+'_vectors.pkl', 'rb'))\n",
    "id2token = pkl.load(open('pickle/'+str(vocab_size)+'_id2token.pkl', 'rb'))\n",
    "token2id = pkl.load(open('pickle/'+str(vocab_size)+'_token2id.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = NNModel(100000, 300, 100000 , 100, 3, \"DIRECT\", is_pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

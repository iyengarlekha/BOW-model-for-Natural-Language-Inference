{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_PATH = os.getcwd()\n",
    "DATA_PATH = '/data/'\n",
    "VEC_PATH = '/wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data\n",
    "from load_data import create_weights, create_emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## added three cat_mode into model function\n",
    "from models import LogisticRegression, NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added training accuracy, need to add training and validation loss later\n",
    "from training import acc, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load raw data sets\n",
    "snli_train = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_train.tsv\", sep='\\t')\n",
    "snli_val = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_val.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess raw datasets\n",
    "train_data = load_data.prepare_data(snli_train)\n",
    "val_data = load_data.prepare_data(snli_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 100\n",
    "MAX_SENTENCE_LENGTH = 30\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASS = 20\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHES = 10\n",
    "\n",
    "VOCAB_SIZES = [5000, 10000, 25000, 50000]\n",
    "EMB_DIMS = [100,200,300,500]\n",
    "CAT_MODES = [\"DIRECT\",\"MUL\",\"SUB\"]\n",
    "MODEL_TYPES = {'log-reg': LogisticRegression,\n",
    "              'neural-net': NeuralNetwork}\n",
    "\n",
    "CRITERION = nn.NLLLoss()\n",
    "SAVE_FOLDER = os.path.join('models', 'snli')\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "results = [] # list of dictionaries that will be converted to DataFrame later\n",
    "for vocab_size in VOCAB_SIZES:\n",
    "    # Load datasets\n",
    "    vectors = pkl.load(open('pickle/'+str(vocab_size)+'_vectors.pkl', 'rb'))\n",
    "    id2token = pkl.load(open('pickle/'+str(vocab_size)+'_id2token.pkl', 'rb'))\n",
    "    token2id = pkl.load(open('pickle/'+str(vocab_size)+'_token2id.pkl', 'rb'))\n",
    "    ## Convert to token lists to lists of corresponding indices\n",
    "    indiced_train_data, train_target = load_data.token2index_dataset(train_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "    indiced_val_data, val_target = load_data.token2index_dataset(val_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "    train_dataset = load_data.SNLIDataset(indiced_train_data, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "    val_dataset = load_data.SNLIDataset(indiced_val_data, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "    \n",
    "    for embed_dim in EMB_DIMS: \n",
    "        num_embed = len(set(id2token))\n",
    "        emb_layer = nn.Embedding(num_embed, embed_dim)\n",
    "        \n",
    "        '''the following line is used for pre-trained embedding'''\n",
    "        #embed_layer, num_embed, embed_dim = create_emb_layer(create_weights(vectors, id2token), non_trainable = True)\n",
    "        \n",
    "        for cat_mode in CAT_MODES:\n",
    "            print('Vocab_size:{}, Embed_dim:{}, cat_mode:{}'.format(vocab_size, embed_dim, cat_mode))\n",
    "                 \n",
    "            for model_str, model_class in MODEL_TYPES.items():\n",
    "                # Generate filename to save model\n",
    "                # Will need to change for pretrained vectors\n",
    "                filename = '{}_{}_{}_{}.pt'.format(vocab_size, embed_dim, cat_mode, model_str)\n",
    "                save_path = os.path.join(SAVE_FOLDER, filename)\n",
    "                \n",
    "                if model_class is NeuralNetwork:\n",
    "                    model = model_class(emb_layer, embed_dim, NUM_CLASS, HIDDEN_DIM, cat_mode)\n",
    "                elif model_class is LogisticRegression:\n",
    "                    model = model_class(emb_layer, embed_dim, NUM_CLASS, cat_mode)\n",
    "                    \n",
    "                criterion = nn.NLLLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "                \n",
    "                print('Training model.\\nVocab Size: {}\\nEmbedding Dimension: {}\\nConcat Mode: {}\\nModel: {}'.format(vocab_size, embed_dim, cat_mode, model_str))\n",
    "                train_output = train_model(model=model,\n",
    "                                          train_loader=train_loader, \n",
    "                                          val_loader=val_loader, \n",
    "                                          optimizer=optimizer, \n",
    "                                          criterion=criterion, \n",
    "                                          n_epochs=NUM_EPOCHES,\n",
    "                                          save_file=save_path)\n",
    "                results.append(vars(train_output))\n",
    "                \n",
    "results_df = pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

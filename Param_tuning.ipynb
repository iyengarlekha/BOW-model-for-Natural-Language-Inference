{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_PATH = os.getcwd()\n",
    "DATA_PATH = '/data/'\n",
    "VEC_PATH = '/wiki-news-300d-1M.vec'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data\n",
    "from load_data import create_weights, create_emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## added three cat_mode into model function\n",
    "from models import LogisticRegression, NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added training accuracy, need to add training and validation loss later\n",
    "from training import acc, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load raw data sets\n",
    "snli_train = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_train.tsv\", sep='\\t')\n",
    "snli_val = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_val.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess raw datasets\n",
    "train_data = load_data.prepare_data(snli_train)\n",
    "val_data = load_data.prepare_data(snli_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRClassifier(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        n_in: Number of features\n",
    "        n_out: Number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up out linear layer. This initializes the weights\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        \n",
    "        # Explicitly initialize the weights with the initialization\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input data [N, k]\n",
    "        ---\n",
    "        Returns: log probabilities of each class [N, c]\n",
    "        \"\"\"\n",
    "        # Apply the linear function to get our logit (real numbers)\n",
    "        logit = self.linear(x)\n",
    "        \n",
    "        # Apply log_softmax to get logs of normalized probabilities\n",
    "        return F.log_softmax(logit, dim=1)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # Use some specific initialization schemes\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.uniform_(self.linear.bias)\n",
    "\n",
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, n_in, h_s, n_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_in,h_s)\n",
    "        self.linear2 = nn.Linear(h_s,h_s)\n",
    "        self.linear3 = nn.Linear(h_s,n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        return F.log_softmax(x)\n",
    "        \n",
    "\n",
    "class BOWEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, class_in):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BOWEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,class_in)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        out = self.linear(out.float())\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, n_in, h_s, n_out, combine_mode):\n",
    "        super().__init__()\n",
    "        self.encoder = BOWEncoder(vocab_size, embed_dim, n_in)\n",
    "        if combine_mode == 'DIRECT':\n",
    "            n_in = n_in * 2;\n",
    "        self.classifier = NNClassifier(n_in,h_s, n_out)\n",
    "    \n",
    "    def forward(self, premise, len_premise, hypothesis, len_hypo, combine_mode):\n",
    "        premise = self.encoder(premise, len_premise)\n",
    "        hypothesis = self.encoder(hypothesis, len_hypo)\n",
    "        if combine_mode == 'DIRECT':\n",
    "            x = torch.cat((premise, hypothesis),1)\n",
    "        elif combine_mode == 'MUL':\n",
    "            x = torch.mul(premise, hypothesis)\n",
    "        elif combine_mode == 'SUB':\n",
    "            x = torch.sub(premise, hypothesis)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, n_in, n_out, combine_mode):\n",
    "        super().__init__()\n",
    "        self.encoder = BOWEncoder(vocab_size, embed_dim, n_in)\n",
    "        if combine_mode == 'DIRECT':\n",
    "            n_in = n_in * 2;\n",
    "        self.classifier = LRClassifier(n_in, n_out)\n",
    "    \n",
    "    def forward(self, premise, len_premise, hypothesis, len_hypo, combine_mode):\n",
    "        premise = self.encoder(premise, len_premise)\n",
    "        hypothesis = self.encoder(hypothesis, len_hypo)\n",
    "        if combine_mode == 'DIRECT':\n",
    "            x = torch.cat((premise, hypothesis),1)\n",
    "        elif combine_mode == 'MUL':\n",
    "            x = torch.mul(premise, hypothesis)\n",
    "        elif combine_mode == 'SUB':\n",
    "            x = torch.sub(premise, hypothesis)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "num_class = 3\n",
    "batch_size = 32\n",
    "h_s = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 30\n",
    "\n",
    "VOCAB_SIZES = [5000, 10000, 20000, 40000,50000]\n",
    "EMB_DIMS = [50, 100,200,300,500]\n",
    "CAT_MODES = [\"DIRECT\",\"MUL\",\"SUB\"]\n",
    "MODEL_TYPES = { 'log-reg': LogisticRegression,\n",
    "              'neural-net': NeuralNetwork}\n",
    "SAVE_FOLDER = os.path.join('models', 'snli')\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size:5000, Embed_dim:50, cat_mode:DIRECT, Classifier:log-reg\n",
      "Epoch: 1 | Train Loss: 0.8465526700019836 | Val Loss: 0.8521768450737 | Val Accuracy: 55.1\n",
      "Epoch: 2 | Train Loss: 0.576822817325592 | Val Loss: 0.6295715570449829 | Val Accuracy: 59.6\n",
      "Epoch: 3 | Train Loss: 0.7618913650512695 | Val Loss: 1.0731502771377563 | Val Accuracy: 59.9\n",
      "Epoch: 4 | Train Loss: 0.8178608417510986 | Val Loss: 0.5459192991256714 | Val Accuracy: 60.3\n",
      "Epoch: 5 | Train Loss: 0.8814212679862976 | Val Loss: 1.366952657699585 | Val Accuracy: 60.8\n",
      "Epoch: 6 | Train Loss: 0.7352291941642761 | Val Loss: 1.39240562915802 | Val Accuracy: 60.9\n",
      "Epoch: 7 | Train Loss: 0.8335384130477905 | Val Loss: 0.8920820355415344 | Val Accuracy: 60.7\n",
      "Epoch: 8 | Train Loss: 0.5597620606422424 | Val Loss: 1.1012122631072998 | Val Accuracy: 60.7\n",
      "Epoch: 9 | Train Loss: 0.7669667601585388 | Val Loss: 0.7370402812957764 | Val Accuracy: 60.8\n",
      "Epoch: 10 | Train Loss: 0.5742509365081787 | Val Loss: 0.8449488282203674 | Val Accuracy: 62.0\n",
      "Vocab_size:5000, Embed_dim:50, cat_mode:DIRECT, Classifier:neural-net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lekha/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 0.8974243402481079 | Val Loss: 1.4534313678741455 | Val Accuracy: 58.6\n",
      "Epoch: 2 | Train Loss: 0.7620750069618225 | Val Loss: 0.8860758543014526 | Val Accuracy: 62.1\n",
      "Epoch: 3 | Train Loss: 0.8816721439361572 | Val Loss: 0.31522682309150696 | Val Accuracy: 62.9\n",
      "Epoch: 4 | Train Loss: 0.5966107845306396 | Val Loss: 0.5474328398704529 | Val Accuracy: 64.2\n",
      "Epoch: 5 | Train Loss: 0.5737042427062988 | Val Loss: 0.9308916926383972 | Val Accuracy: 65.4\n",
      "Epoch: 6 | Train Loss: 0.6611303091049194 | Val Loss: 1.0396567583084106 | Val Accuracy: 65.9\n",
      "Epoch: 7 | Train Loss: 0.5473144054412842 | Val Loss: 0.8822601437568665 | Val Accuracy: 66.9\n",
      "Epoch: 8 | Train Loss: 0.5910141468048096 | Val Loss: 0.9317623376846313 | Val Accuracy: 66.9\n",
      "Epoch: 9 | Train Loss: 0.5002008676528931 | Val Loss: 0.9600455164909363 | Val Accuracy: 65.4\n",
      "Epoch: 10 | Train Loss: 0.6141411066055298 | Val Loss: 0.7764148116111755 | Val Accuracy: 64.5\n",
      "Vocab_size:5000, Embed_dim:50, cat_mode:MUL, Classifier:log-reg\n",
      "Epoch: 1 | Train Loss: 1.0516828298568726 | Val Loss: 1.391865849494934 | Val Accuracy: 56.2\n",
      "Epoch: 2 | Train Loss: 0.9768803715705872 | Val Loss: 0.8482702970504761 | Val Accuracy: 61.3\n",
      "Epoch: 3 | Train Loss: 0.681318461894989 | Val Loss: 0.7104842662811279 | Val Accuracy: 61.8\n",
      "Epoch: 4 | Train Loss: 0.6899335384368896 | Val Loss: 0.8348270058631897 | Val Accuracy: 62.7\n",
      "Epoch: 5 | Train Loss: 1.032476544380188 | Val Loss: 1.0258797407150269 | Val Accuracy: 63.2\n",
      "Epoch: 6 | Train Loss: 0.8859768509864807 | Val Loss: 1.0339242219924927 | Val Accuracy: 63.3\n",
      "Epoch: 7 | Train Loss: 0.6575716733932495 | Val Loss: 0.570066511631012 | Val Accuracy: 62.0\n",
      "Epoch: 8 | Train Loss: 0.8246663212776184 | Val Loss: 0.526835560798645 | Val Accuracy: 63.0\n",
      "Epoch: 9 | Train Loss: 1.0053813457489014 | Val Loss: 0.6965252161026001 | Val Accuracy: 62.8\n",
      "Epoch: 10 | Train Loss: 0.4519633650779724 | Val Loss: 1.0581291913986206 | Val Accuracy: 63.1\n",
      "Vocab_size:5000, Embed_dim:50, cat_mode:MUL, Classifier:neural-net\n",
      "Epoch: 1 | Train Loss: 0.854844331741333 | Val Loss: 1.1699156761169434 | Val Accuracy: 57.5\n",
      "Epoch: 2 | Train Loss: 0.6654289364814758 | Val Loss: 0.7067548036575317 | Val Accuracy: 60.4\n",
      "Epoch: 3 | Train Loss: 0.7871416211128235 | Val Loss: 0.8906860947608948 | Val Accuracy: 62.2\n",
      "Epoch: 4 | Train Loss: 0.7980901002883911 | Val Loss: 0.7254447340965271 | Val Accuracy: 62.3\n"
     ]
    }
   ],
   "source": [
    "for vocab_size in VOCAB_SIZES:\n",
    "    # Load datasets\n",
    "    vectors = pkl.load(open('pickle/'+str(vocab_size)+'_vectors.pkl', 'rb'))\n",
    "    id2token = pkl.load(open('pickle/'+str(vocab_size)+'_id2token.pkl', 'rb'))\n",
    "    token2id = pkl.load(open('pickle/'+str(vocab_size)+'_token2id.pkl', 'rb'))\n",
    "    ## Convert to token lists to lists of corresponding indices\n",
    "    indiced_train_data, train_target = load_data.token2index_dataset(train_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "    indiced_val_data, val_target = load_data.token2index_dataset(val_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "    train_dataset = load_data.SNLIDataset(indiced_train_data, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "    val_dataset = load_data.SNLIDataset(indiced_val_data, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "    \n",
    "    for embed_dim in EMB_DIMS: \n",
    "        n_in = len(id2token)\n",
    "        \n",
    "        for cat_mode in CAT_MODES:\n",
    "            \n",
    "            for model_str, model_class in MODEL_TYPES.items():\n",
    "                print('Vocab_size:{}, Embed_dim:{}, cat_mode:{}, Classifier:{}'.format(vocab_size, embed_dim, cat_mode, model_str))\n",
    "                filename = '{}_{}_{}_{}.pt'.format(vocab_size, embed_dim, cat_mode, model_str)\n",
    "                save_path = os.path.join(SAVE_FOLDER, filename)\n",
    "                if model_class is NeuralNetwork:\n",
    "                    model = NNModel(vocab_size, embed_dim, n_in , h_s, num_class, cat_mode)\n",
    "                elif model_class is LogisticRegression:\n",
    "                    model = LRModel(vocab_size, embed_dim, n_in, num_class, cat_mode)                \n",
    "\n",
    "                model.to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "                num_epochs = 10\n",
    "                history_train_acc, history_val_acc, history_train_loss, history_val_loss = [], [], [], []\n",
    "                best_accuracy = 0\n",
    "                \n",
    "                for epoch in range(num_epochs):\n",
    "                    for i, (premise, len_premise, hypothesis, len_hypo, labels) in enumerate(train_loader):\n",
    "                        model.train()\n",
    "                        # Load samples\n",
    "                        premise = premise.to(device)\n",
    "                        hypothesis = hypothesis.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(premise, len_premise, hypothesis, len_hypo, cat_mode )\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if i > 0 and i % 500 == 0:\n",
    "                            train_loss = loss.data.item()\n",
    "\n",
    "                            model.eval()\n",
    "                            correct = 0\n",
    "                            total = 0\n",
    "\n",
    "                            for premise_val, len_premise_val, hypothesis_val, len_hypo_val, labels_val in val_loader:\n",
    "                                # Load samples\n",
    "                                premise_val = premise_val.to(device)\n",
    "                                hypothesis_val = hypothesis_val.to(device)\n",
    "                                labels_val = labels_val.to(device)\n",
    "\n",
    "                                outputs_val = model(premise_val, len_premise_val, hypothesis_val, len_hypo_val, cat_mode)\n",
    "                \n",
    "                                val_loss = criterion(outputs_val, labels_val)\n",
    "                                predicted = outputs_val.max(1, keepdim=True)[1]\n",
    "                                total += labels_val.size(0)\n",
    "                                # Total correct predictions\n",
    "                                correct += predicted.eq(labels_val.view_as(predicted)).sum().item()\n",
    "            \n",
    "                            accuracy = 100. * correct / total\n",
    "#                             print('Iter: {} | Train Loss: {} | Val Loss: {} | Val Accuracy: {}'.format(i, train_loss, val_loss.item(), round(accuracy, 2)))\n",
    "                            # Append to history\n",
    "                            history_val_loss.append(val_loss.data.item())\n",
    "                            history_val_acc.append(round(accuracy, 2))\n",
    "                            history_train_loss.append(train_loss)                \n",
    "                            # Save model when accuracy beats best accuracy\n",
    "                            if accuracy > best_accuracy:\n",
    "                                best_accuracy = accuracy\n",
    "                                torch.save(model.state_dict(), save_path)\n",
    "                    print('Epoch: {} | Train Loss: {} | Val Loss: {} | Val Accuracy: {}'.format((epoch+1), train_loss, val_loss.item(), round(accuracy, 2)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

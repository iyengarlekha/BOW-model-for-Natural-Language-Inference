{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_PATH = os.getcwd()\n",
    "DATA_PATH = '/data/'\n",
    "VEC_PATH = '/wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data\n",
    "from load_data import create_weights, create_emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## added three cat_mode into model function\n",
    "from models import LogisticRegression, NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added training accuracy, need to add training and validation loss later\n",
    "from training import acc, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load raw data sets\n",
    "snli_train = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_train.tsv\", sep='\\t')\n",
    "snli_val = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_val.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess raw datasets\n",
    "train_data = load_data.prepare_data(snli_train)\n",
    "val_data = load_data.prepare_data(snli_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 100\n",
    "MAX_SENTENCE_LENGTH = 30\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASS = 20\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHES = 10\n",
    "\n",
    "VOCAB_SIZES = [10000, 20000, 40000]\n",
    "EMB_DIMS = [100,200,300,500]\n",
    "CAT_MODES = [\"DIRECT\",\"MUL\",\"SUB\"]\n",
    "MODEL_TYPES = {'log-reg': LogisticRegression,\n",
    "              'neural-net': NeuralNetwork}\n",
    "\n",
    "CRITERION = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size:50000, Embed_dim:100, cat_mode:DIRECT\n",
      "Training model.\n",
      "Vocab Size: 50000\n",
      "Embedding Dimension: 100\n",
      "Concat Mode: DIRECT\n",
      "Model: log-reg\n",
      "Starting epoch 0\n",
      "Epoch: [1/1], Step: [500/3125],Training Loss: 1.0192272663116455, Validation Acc: 52.1, Time: 25.08227777481079 sec\n",
      "Epoch: [1/1], Step: [1000/3125],Training Loss: 1.0992815494537354, Validation Acc: 56.9, Time: 51.9400954246521 sec\n",
      "Epoch: [1/1], Step: [1500/3125],Training Loss: 0.9212397933006287, Validation Acc: 60.1, Time: 85.02800846099854 sec\n",
      "Epoch: [1/1], Step: [2000/3125],Training Loss: 0.8241577744483948, Validation Acc: 60.3, Time: 123.95779061317444 sec\n",
      "Epoch: [1/1], Step: [2500/3125],Training Loss: 0.9588789939880371, Validation Acc: 61.5, Time: 166.792498588562 sec\n",
      "Epoch: [1/1], Step: [3000/3125],Training Loss: 0.9739933609962463, Validation Acc: 59.7, Time: 215.07045888900757 sec\n",
      "End of epoch 1, Training Acc: 65.23,Validation Acc: 60.5, Time: 236.84411311149597 sec\n",
      "New best model found, saving at models/snli/50000_100_DIRECT_log-reg.pt\n",
      "\n",
      "Training model.\n",
      "Vocab Size: 50000\n",
      "Embedding Dimension: 100\n",
      "Concat Mode: DIRECT\n",
      "Model: neural-net\n",
      "Starting epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0616468ca929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m                                           \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                           \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                           save_file=save_path)\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nyu/ds-1011/hw/NLP_Fall2019_Assignment1/training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, n_epochs, save_file)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "results = [] # list of dictionaries that will be converted to DataFrame later\n",
    "for vocab_size in VOCAB_SIZES:\n",
    "    # Load datasets\n",
    "    vectors = pkl.load(open('pickle/'+str(vocab_size)+'_vectors.pkl', 'rb'))\n",
    "    id2token = pkl.load(open('pickle/'+str(vocab_size)+'_id2token.pkl', 'rb'))\n",
    "    token2id = pkl.load(open('pickle/'+str(vocab_size)+'_token2id.pkl', 'rb'))\n",
    "    ## Convert to token lists to lists of corresponding indices\n",
    "    indiced_train_data, train_target = load_data.token2index_dataset(train_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "    indiced_val_data, val_target = load_data.token2index_dataset(val_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "    train_dataset = load_data.SNLIDataset(indiced_train_data, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "    val_dataset = load_data.SNLIDataset(indiced_val_data, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "    \n",
    "    for embed_dim in EMB_DIMS: \n",
    "        num_embed = len(set(id2token))\n",
    "        emb_layer = nn.Embedding(num_embed, embed_dim)\n",
    "        \n",
    "        '''the following line is used for pre-trained embedding'''\n",
    "        #embed_layer, num_embed, embed_dim = create_emb_layer(create_weights(vectors, id2token), non_trainable = True)\n",
    "        \n",
    "        for cat_mode in CAT_MODES:\n",
    "            print('Vocab_size:{}, Embed_dim:{}, cat_mode:{}'.format(vocab_size, embed_dim, cat_mode))\n",
    "                 \n",
    "            for model_str, model_class in MODEL_TYPES.items():\n",
    "                # Generate filename to save model\n",
    "                # Will need to change for pretrained vectors\n",
    "                filename = '{}_{}_{}_{}.pt'.format(vocab_size, embed_dim, cat_mode, model_str)\n",
    "                save_path = os.path.join('models', 'snli', filename)\n",
    "                \n",
    "                if model_class is NeuralNetwork:\n",
    "                    model = model_class(emb_layer, embed_dim, NUM_CLASS, HIDDEN_DIM, cat_mode)\n",
    "                elif model_class is LogisticRegression:\n",
    "                    model = model_class(emb_layer, embed_dim, NUM_CLASS, cat_mode)\n",
    "                    \n",
    "                criterion = nn.NLLLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "                \n",
    "                print('Training model.\\nVocab Size: {}\\nEmbedding Dimension: {}\\nConcat Mode: {}\\nModel: {}'.format(vocab_size, embed_dim, cat_mode, model_str))\n",
    "                train_output = train_model(model=model,\n",
    "                                          train_loader=train_loader, \n",
    "                                          val_loader=val_loader, \n",
    "                                          optimizer=optimizer, \n",
    "                                          criterion=criterion, \n",
    "                                          n_epochs=NUM_EPOCHES,\n",
    "                                          save_file=save_path)\n",
    "                results.append(vars(train_output))\n",
    "                \n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch]",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_PATH = os.getcwd()\n",
    "DATA_PATH = '/data/'\n",
    "VEC_PATH = '/wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data\n",
    "from load_data import create_weights, create_emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load raw data sets\n",
    "snli_train = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_train.tsv\", sep='\\t')\n",
    "snli_val = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_val.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess raw datasets\n",
    "train_data = load_data.prepare_data(snli_train)\n",
    "val_data = load_data.prepare_data(snli_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL CLASSES\n",
    "\n",
    "class LRClassifier(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        n_in: Number of features\n",
    "        n_out: Number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set up out linear layer. This initializes the weights\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        \n",
    "        # Explicitly initialize the weights with the initialization\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input data [N, k]\n",
    "        ---\n",
    "        Returns: log probabilities of each class [N, c]\n",
    "        \"\"\"\n",
    "        # Apply the linear function to get our logit (real numbers)\n",
    "        logit = self.linear(x)\n",
    "        \n",
    "        # Apply log_softmax to get logs of normalized probabilities\n",
    "        return F.log_softmax(logit, dim=1)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # Use some specific initialization schemes\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.uniform_(self.linear.bias)\n",
    "\n",
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, n_in, h_s, n_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_in,h_s)\n",
    "        self.linear2 = nn.Linear(h_s,h_s)\n",
    "        self.linear3 = nn.Linear(h_s,n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        return F.log_softmax(x)\n",
    "        \n",
    "\n",
    "class BOWEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, class_in):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BOWEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,class_in)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        out = self.linear(out.float())\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, n_in, h_s, n_out, combine_mode):\n",
    "        super().__init__()\n",
    "        self.encoder = BOWEncoder(vocab_size, embed_dim, n_in)\n",
    "        if combine_mode == 'DIRECT':\n",
    "            n_in = n_in * 2;\n",
    "        self.classifier = NNClassifier(n_in,h_s, n_out)\n",
    "    \n",
    "    def forward(self, premise, len_premise, hypothesis, len_hypo, combine_mode):\n",
    "        premise = self.encoder(premise, len_premise)\n",
    "        hypothesis = self.encoder(hypothesis, len_hypo)\n",
    "        if combine_mode == 'DIRECT':\n",
    "            x = torch.cat((premise, hypothesis),1)\n",
    "        elif combine_mode == 'MUL':\n",
    "            x = torch.mul(premise, hypothesis)\n",
    "        elif combine_mode == 'SUB':\n",
    "            x = torch.sub(premise, hypothesis)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, n_in, n_out, combine_mode):\n",
    "        super().__init__()\n",
    "        self.encoder = BOWEncoder(vocab_size, embed_dim, n_in)\n",
    "        if combine_mode == 'DIRECT':\n",
    "            n_in = n_in * 2;\n",
    "        self.classifier = LRClassifier(n_in, n_out)\n",
    "    \n",
    "    def forward(self, premise, len_premise, hypothesis, len_hypo, combine_mode):\n",
    "        premise = self.encoder(premise, len_premise)\n",
    "        hypothesis = self.encoder(hypothesis, len_hypo)\n",
    "        if combine_mode == 'DIRECT':\n",
    "            x = torch.cat((premise, hypothesis),1)\n",
    "        elif combine_mode == 'MUL':\n",
    "            x = torch.mul(premise, hypothesis)\n",
    "        elif combine_mode == 'SUB':\n",
    "            x = torch.sub(premise, hypothesis)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "num_class = 3\n",
    "batch_size = 32\n",
    "h_s = 100 # size of hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 30\n",
    "VOCAB_SIZES = [5000, 10000, 20000, 40000,50000]\n",
    "EMB_DIMS = [50, 100,200,300,500]\n",
    "CAT_MODES = [\"DIRECT\",\"MUL\",\"SUB\"]\n",
    "\n",
    "MODEL_TYPES = [ 'lr','nn']\n",
    "SAVE_FOLDER = os.path.join('models', 'snli')\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter tuning on SNLI\n",
    "\n",
    "for vocab_size in VOCAB_SIZES:\n",
    "    # Load datasets\n",
    "    vectors = pkl.load(open('pickle/'+str(vocab_size)+'_vectors.pkl', 'rb'))\n",
    "    id2token = pkl.load(open('pickle/'+str(vocab_size)+'_id2token.pkl', 'rb'))\n",
    "    token2id = pkl.load(open('pickle/'+str(vocab_size)+'_token2id.pkl', 'rb'))\n",
    "    ## Convert to token lists to lists of corresponding indices\n",
    "    indiced_train_data, train_target = load_data.token2index_dataset(train_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "    indiced_val_data, val_target = load_data.token2index_dataset(val_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "    train_dataset = load_data.SNLIDataset(indiced_train_data, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "    val_dataset = load_data.SNLIDataset(indiced_val_data, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "    \n",
    "    for embed_dim in EMB_DIMS: \n",
    "        n_in = len(id2token)\n",
    "        \n",
    "        for cat_mode in CAT_MODES:\n",
    "            \n",
    "            for model_str in MODEL_TYPES:\n",
    "                print('Vocab_size:{}, Embed_dim:{}, cat_mode:{}, Classifier:{}'.format(vocab_size, embed_dim, cat_mode, model_str))\n",
    "                filename = '{}_{}_{}_{}.pt'.format(vocab_size, embed_dim, cat_mode, model_str)\n",
    "                save_path = os.path.join(SAVE_FOLDER, filename)\n",
    "                if model_str is 'nn':\n",
    "                    model = NNModel(vocab_size, embed_dim, n_in , h_s, num_class, cat_mode)\n",
    "                elif model_str is 'lr':\n",
    "                    model = LRModel(vocab_size, embed_dim, n_in, num_class, cat_mode)                \n",
    "\n",
    "                model.to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "                num_epochs = 10\n",
    "                history_train_acc, history_val_acc, history_train_loss, history_val_loss = [], [], [], []\n",
    "                best_accuracy = 0\n",
    "                \n",
    "                for epoch in range(num_epochs):\n",
    "                    for i, (premise, len_premise, hypothesis, len_hypo, labels) in enumerate(train_loader):\n",
    "                        model.train()\n",
    "                        # Load samples\n",
    "                        premise = premise.to(device)\n",
    "                        hypothesis = hypothesis.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(premise, len_premise, hypothesis, len_hypo, cat_mode )\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if i > 0 and i % 500 == 0:\n",
    "                            train_loss = loss.data.item()\n",
    "\n",
    "                            model.eval()\n",
    "                            correct = 0\n",
    "                            total = 0\n",
    "\n",
    "                            for premise_val, len_premise_val, hypothesis_val, len_hypo_val, labels_val in val_loader:\n",
    "                                # Load samples\n",
    "                                premise_val = premise_val.to(device)\n",
    "                                hypothesis_val = hypothesis_val.to(device)\n",
    "                                labels_val = labels_val.to(device)\n",
    "\n",
    "                                outputs_val = model(premise_val, len_premise_val, hypothesis_val, len_hypo_val, cat_mode)\n",
    "                \n",
    "                                val_loss = criterion(outputs_val, labels_val)\n",
    "                                predicted = outputs_val.max(1, keepdim=True)[1]\n",
    "                                total += labels_val.size(0)\n",
    "                                # Total correct predictions\n",
    "                                correct += predicted.eq(labels_val.view_as(predicted)).sum().item()\n",
    "            \n",
    "                            accuracy = 100. * correct / total\n",
    "#                             print('Iter: {} | Train Loss: {} | Val Loss: {} | Val Accuracy: {}'.format(i, train_loss, val_loss.item(), round(accuracy, 2)))\n",
    "                            # Append to history\n",
    "                            history_val_loss.append(val_loss.data.item())\n",
    "                            history_val_acc.append(round(accuracy, 2))\n",
    "                            history_train_loss.append(train_loss)                \n",
    "                            # Save model when accuracy beats best accuracy\n",
    "                            if accuracy > best_accuracy:\n",
    "                                best_accuracy = accuracy\n",
    "                                torch.save(model.state_dict(), save_path)\n",
    "                    print('Epoch: {} | Train Loss: {} | Val Loss: {} | Val Accuracy: {}'.format((epoch+1), train_loss, val_loss.item(), round(accuracy, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

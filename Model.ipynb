{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "#### Date: 09/14/2019\n",
    "#### Implement Logistic Regression and Neural Network (2-layer) to the embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The customized util package\n",
    "import load_data\n",
    "from load_data import create_weights, create_emb_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_PATH = os.getcwd()\n",
    "\n",
    "DATA_PATH = '/data/'\n",
    "VEC_PATH = '/wiki-news-300d-1M.vec'\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 100\n",
    "MAX_SENTENCE_LENGTH = 30\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASS = 20\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHES = 5\n",
    "CONCAT_MODE = \"DIRECT\" ## Possible values: \"DIRECT\", \"AVERAGE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load raw data sets\n",
    "snli_train = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_train.tsv\", sep='\\t')\n",
    "snli_val = pd.read_csv(CURR_PATH + DATA_PATH + \"snli_val.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess raw date sets\n",
    "train_data = load_data.prepare_data(snli_train)\n",
    "val_data = load_data.prepare_data(snli_val)\n",
    "vectors = pkl.load(open('pickle/'+str(VOCAB_SIZE)+'_vectors.pkl', 'rb'))\n",
    "id2token = pkl.load(open('pickle/'+str(VOCAB_SIZE)+'_id2token.pkl', 'rb'))\n",
    "token2id = pkl.load(open('pickle/'+str(VOCAB_SIZE)+'_token2id.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to token lists to lists of corresponding indices\n",
    "indiced_train_data, train_target = load_data.token2index_dataset(train_data, token2id, MAX_SENTENCE_LENGTH)\n",
    "indiced_val_data, val_target = load_data.token2index_dataset(val_data, token2id, MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, data, target):\n",
    "        \"\"\"\n",
    "        @param data: dict, with `sentence1` and `sentence2` columns consist of indiced data\n",
    "        @param target: list \n",
    "        \"\"\"\n",
    "        self.x1 = data['sentence1']\n",
    "        self.x2 = data['sentence2']\n",
    "        self.y = target\n",
    "        assert (len(self.x1) == len(self.x2) == len(self.y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        Turn sentence1[key] and sentence2[key] into indices\n",
    "        \"\"\"\n",
    "        token_idx_1 = self.x1[key]\n",
    "        token_idx_2 = self.x2[key]\n",
    "                \n",
    "        label = self.y[key]\n",
    "        return [token_idx_1, len(token_idx_1), token_idx_2, len(token_idx_2), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNLI_collate_func(batch):\n",
    "    \"\"\"Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    x1_list = []\n",
    "    x1_length_list = []\n",
    "    x2_list = []\n",
    "    x2_length_list = []\n",
    "    label_list = []\n",
    "    for datum in batch:\n",
    "        x1_padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        x1_list.append(x1_padded_vec)\n",
    "        x1_length_list.append(datum[1])\n",
    "        \n",
    "        x2_padded_vec = np.pad(np.array(datum[2]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        x2_list.append(x2_padded_vec)\n",
    "        x2_length_list.append(datum[3])\n",
    "        \n",
    "        label_list.append(datum[4])\n",
    "\n",
    "    return [torch.from_numpy(np.array(x1_list)), torch.LongTensor(x1_length_list),\n",
    "            torch.from_numpy(np.array(x2_list)), torch.LongTensor(x2_length_list),\n",
    "            torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_data.SNLIDataset(indiced_train_data, train_target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = load_data.SNLIDataset(indiced_val_data, val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=load_data.SNLI_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    '''\n",
    "    Logistic regression classification model\n",
    "    '''\n",
    "    def __init__(self, embed_layer, embed_dim, num_class, cat_mode='DIRECT'):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        self.embed = embed_layer\n",
    "        self.cat_model = cat_mode\n",
    "        if cat_mode == 'DIRECT':\n",
    "            self.linear = nn.Linear(2*embed_dim, num_class)\n",
    "        else:\n",
    "            self.linear = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, data_pre, data_post, len_pre, len_post):\n",
    "        out_pre = self.embed(data_pre)\n",
    "        out_pre = torch.sum(out_pre, dim=1)\n",
    "        out_pre /= len_pre.view(len_pre.size()[0],1).expand_as(out_pre).float()\n",
    "        out_post = self.embed(data_post)\n",
    "        out_post = torch.sum(out_post, dim=1)\n",
    "        out_post /= len_post.view(len_post.size()[0],1).expand_as(out_post).float()\n",
    "        if self.cat_model == 'DIRECT':\n",
    "            out = torch.cat((out_pre, out_post), 1)\n",
    "            logit = self.linear(out)\n",
    "            return F.log_softmax(logit, dim=1)\n",
    "        else:\n",
    "            out = out_pre.add(out_post)\n",
    "            out = torch.div(out, 2.0)\n",
    "            logit = self.linear(out)\n",
    "            return F.log_softmax(logit, dim=1)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # Use some specific initialization schemes\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.uniform_(self.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_lr(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_pre, len_pre, data_post, len_post, labels in loader:\n",
    "        outputs = model(data_pre, data_post, len_pre, len_post)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pre_trained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer, num_embed, embed_dim = create_emb_layer(create_weights(vectors, id2token), non_trainable = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(embed_layer, embed_dim, NUM_CLASS, 'DIRECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.Adam(lr_model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [500/3125], Validation Acc: 54.2, Time: 2.26324200630188 sec\n",
      "Epoch: [1/5], Step: [1000/3125], Validation Acc: 55.6, Time: 4.430830240249634 sec\n",
      "Epoch: [1/5], Step: [1500/3125], Validation Acc: 55.9, Time: 6.595863103866577 sec\n",
      "Epoch: [1/5], Step: [2000/3125], Validation Acc: 58.1, Time: 8.963868141174316 sec\n",
      "Epoch: [1/5], Step: [2500/3125], Validation Acc: 57.2, Time: 11.220885992050171 sec\n",
      "Epoch: [1/5], Step: [3000/3125], Validation Acc: 57.0, Time: 13.408932209014893 sec\n",
      "Epoch: [2/5], Step: [500/3125], Validation Acc: 57.1, Time: 16.179883003234863 sec\n",
      "Epoch: [2/5], Step: [1000/3125], Validation Acc: 57.9, Time: 18.47706437110901 sec\n",
      "Epoch: [2/5], Step: [1500/3125], Validation Acc: 57.4, Time: 20.72262930870056 sec\n",
      "Epoch: [2/5], Step: [2000/3125], Validation Acc: 57.9, Time: 22.90026021003723 sec\n",
      "Epoch: [2/5], Step: [2500/3125], Validation Acc: 57.4, Time: 25.146512269973755 sec\n",
      "Epoch: [2/5], Step: [3000/3125], Validation Acc: 57.0, Time: 27.330843210220337 sec\n",
      "Epoch: [3/5], Step: [500/3125], Validation Acc: 55.8, Time: 30.035624265670776 sec\n",
      "Epoch: [3/5], Step: [1000/3125], Validation Acc: 55.9, Time: 32.5507173538208 sec\n",
      "Epoch: [3/5], Step: [1500/3125], Validation Acc: 56.7, Time: 35.15241599082947 sec\n",
      "Epoch: [3/5], Step: [2000/3125], Validation Acc: 56.1, Time: 37.46989107131958 sec\n",
      "Epoch: [3/5], Step: [2500/3125], Validation Acc: 58.1, Time: 39.59493398666382 sec\n",
      "Epoch: [3/5], Step: [3000/3125], Validation Acc: 58.2, Time: 41.86406230926514 sec\n",
      "Epoch: [4/5], Step: [500/3125], Validation Acc: 57.5, Time: 44.62361025810242 sec\n",
      "Epoch: [4/5], Step: [1000/3125], Validation Acc: 56.4, Time: 46.81784915924072 sec\n",
      "Epoch: [4/5], Step: [1500/3125], Validation Acc: 57.8, Time: 48.98363709449768 sec\n",
      "Epoch: [4/5], Step: [2000/3125], Validation Acc: 56.1, Time: 51.172330141067505 sec\n",
      "Epoch: [4/5], Step: [2500/3125], Validation Acc: 55.7, Time: 53.39013409614563 sec\n",
      "Epoch: [4/5], Step: [3000/3125], Validation Acc: 58.0, Time: 55.56197118759155 sec\n",
      "Epoch: [5/5], Step: [500/3125], Validation Acc: 57.1, Time: 58.2404899597168 sec\n",
      "Epoch: [5/5], Step: [1000/3125], Validation Acc: 57.1, Time: 60.41411828994751 sec\n",
      "Epoch: [5/5], Step: [1500/3125], Validation Acc: 56.2, Time: 62.59321331977844 sec\n",
      "Epoch: [5/5], Step: [2000/3125], Validation Acc: 57.2, Time: 65.04642033576965 sec\n",
      "Epoch: [5/5], Step: [2500/3125], Validation Acc: 58.2, Time: 67.47350406646729 sec\n",
      "Epoch: [5/5], Step: [3000/3125], Validation Acc: 58.5, Time: 69.95041608810425 sec\n",
      "Done in 70.51167511940002 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    for i, (data_pre, len_pre, data_post, len_post, label) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        lr_model.train()\n",
    "        \n",
    "        y_hat = lr_model(data_pre, data_post, len_pre, len_post)\n",
    "        \n",
    "        train_loss = criterion(y_hat, label)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 500 == 0:\n",
    "            val_acc = test_lr(val_loader, lr_model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Time: {} sec'.format( \n",
    "                       epoch+1, NUM_EPOCHES, i+1, len(train_loader), val_acc, time.time()-start))\n",
    "print(\"Done in {} sec\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''\n",
    "    Neural Network classification model\n",
    "    '''\n",
    "    def __init__(self, embed_layer, embed_dim, num_class, hidden_dim, cat_mode='DIRECT'):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.embed = embed_layer\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.cat_model = cat_mode\n",
    "        if cat_mode == 'DIRECT':\n",
    "            self.linear1 = nn.Linear(2*embed_dim, hidden_dim)\n",
    "            self.linear2 = nn.Linear(hidden_dim, num_class)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
    "            self.linear2 = nn.Linear(hidden_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, data_pre, data_post, len_pre, len_post):\n",
    "        out_pre = self.embed(data_pre)\n",
    "        out_pre = torch.sum(out_pre, dim=1)\n",
    "        out_pre /= len_pre.view(len_pre.size()[0],1).expand_as(out_pre).float()\n",
    "        out_post = self.embed(data_post)\n",
    "        out_post = torch.sum(out_post, dim=1)\n",
    "        out_post /= len_post.view(len_post.size()[0],1).expand_as(out_post).float()\n",
    "        if self.cat_model == 'DIRECT':\n",
    "            out = torch.cat((out_pre, out_post), 1)\n",
    "            z1 = self.linear1(out)\n",
    "            a1 = F.relu(z1)\n",
    "            logit = self.linear2(a1)\n",
    "            return F.log_softmax(logit, dim=1)\n",
    "        else:\n",
    "            out = out_pre.add(out_post)\n",
    "            out = torch.div(out,2)\n",
    "            z1 = self.linear1(out)\n",
    "            a1 = torch.relu(z1)\n",
    "            logit = self.linear2(a1)\n",
    "            return F.log_softmax(logit, dim=1)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # Use some specific initialization schemes\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.uniform_(self.linear1.bias)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        nn.init.uniform_(self.linear2.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_nn(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_pre, len_pre, data_post, len_post, labels in loader:\n",
    "        outputs = model(data_pre, data_post, len_pre, len_post)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pre_trained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer, num_embed, embed_dim = create_emb_layer(create_weights(vectors, id2token), non_trainable = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = NeuralNetwork(embed_layer, embed_dim, NUM_CLASS, HIDDEN_DIM, 'DIRECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [500/3125], Validation Acc: 52.1, Time: 2.5380640029907227 sec\n",
      "Epoch: [1/5], Step: [1000/3125], Validation Acc: 55.0, Time: 5.583797931671143 sec\n",
      "Epoch: [1/5], Step: [1500/3125], Validation Acc: 55.7, Time: 8.719244956970215 sec\n",
      "Epoch: [1/5], Step: [2000/3125], Validation Acc: 55.6, Time: 11.921544075012207 sec\n",
      "Epoch: [1/5], Step: [2500/3125], Validation Acc: 60.3, Time: 15.1805100440979 sec\n",
      "Epoch: [1/5], Step: [3000/3125], Validation Acc: 58.5, Time: 18.396465063095093 sec\n",
      "Epoch: [2/5], Step: [500/3125], Validation Acc: 59.1, Time: 22.46944808959961 sec\n",
      "Epoch: [2/5], Step: [1000/3125], Validation Acc: 61.1, Time: 25.95941400527954 sec\n",
      "Epoch: [2/5], Step: [1500/3125], Validation Acc: 61.2, Time: 29.487083911895752 sec\n",
      "Epoch: [2/5], Step: [2000/3125], Validation Acc: 62.6, Time: 32.767805099487305 sec\n",
      "Epoch: [2/5], Step: [2500/3125], Validation Acc: 63.0, Time: 36.29167294502258 sec\n",
      "Epoch: [2/5], Step: [3000/3125], Validation Acc: 62.5, Time: 39.74349308013916 sec\n",
      "Epoch: [3/5], Step: [500/3125], Validation Acc: 63.4, Time: 44.111658811569214 sec\n",
      "Epoch: [3/5], Step: [1000/3125], Validation Acc: 63.7, Time: 47.63656997680664 sec\n",
      "Epoch: [3/5], Step: [1500/3125], Validation Acc: 59.7, Time: 51.22918391227722 sec\n",
      "Epoch: [3/5], Step: [2000/3125], Validation Acc: 65.1, Time: 55.036324977874756 sec\n",
      "Epoch: [3/5], Step: [2500/3125], Validation Acc: 64.1, Time: 58.63293194770813 sec\n",
      "Epoch: [3/5], Step: [3000/3125], Validation Acc: 63.6, Time: 62.164551973342896 sec\n",
      "Epoch: [4/5], Step: [500/3125], Validation Acc: 64.0, Time: 66.66090106964111 sec\n",
      "Epoch: [4/5], Step: [1000/3125], Validation Acc: 65.4, Time: 70.28324294090271 sec\n",
      "Epoch: [4/5], Step: [1500/3125], Validation Acc: 65.3, Time: 73.9615740776062 sec\n",
      "Epoch: [4/5], Step: [2000/3125], Validation Acc: 64.1, Time: 77.60291504859924 sec\n",
      "Epoch: [4/5], Step: [2500/3125], Validation Acc: 63.8, Time: 81.44144701957703 sec\n",
      "Epoch: [4/5], Step: [3000/3125], Validation Acc: 66.0, Time: 85.18239712715149 sec\n",
      "Epoch: [5/5], Step: [500/3125], Validation Acc: 63.1, Time: 89.81266593933105 sec\n",
      "Epoch: [5/5], Step: [1000/3125], Validation Acc: 65.1, Time: 93.62286496162415 sec\n",
      "Epoch: [5/5], Step: [1500/3125], Validation Acc: 64.8, Time: 97.71043395996094 sec\n",
      "Epoch: [5/5], Step: [2000/3125], Validation Acc: 61.0, Time: 101.37965798377991 sec\n",
      "Epoch: [5/5], Step: [2500/3125], Validation Acc: 65.8, Time: 105.11721992492676 sec\n",
      "Epoch: [5/5], Step: [3000/3125], Validation Acc: 66.2, Time: 109.1064190864563 sec\n",
      "Done in 110.00993919372559 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    for i, (data_pre, len_pre, data_post, len_post, label) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        nn_model.train()\n",
    "        \n",
    "        y_hat = nn_model(data_pre, data_post, len_pre, len_post)\n",
    "        \n",
    "        train_loss = criterion(y_hat, label)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 500 == 0:\n",
    "            val_acc = test_lr(val_loader, nn_model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Time: {} sec'.format( \n",
    "                       epoch+1, NUM_EPOCHES, i+1, len(train_loader), val_acc, time.time()-start))\n",
    "print(\"Done in {} sec\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
